{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuition\n",
    "\n",
    "    1. Expression: \n",
    "        y = b0 + b1*x1 + b2*x2 + ..... + bn*xn\n",
    "        \n",
    "    2. Assumptions:\n",
    "        a. Linearity\n",
    "        b. Homoscendasticity\n",
    "        c. Multivariate Normality\n",
    "        d. Independence of Errors\n",
    "        e. Lack of multicollinearity\n",
    "        \n",
    "    3. Dummy Variables\n",
    "        y = b0 + b1*(R&DSpend) + b2*(Admin) + b3*(Marketing) + b4*(State)\n",
    "        State? How to multiply b4 with some non-numeric stuff? or with categorical values?\n",
    "        Ans: Create dummy variables\n",
    "            Note: Donot include \"All\" of the columns of your dummy varibles into the regression. \n",
    "                  Always emit one dummy variable.\n",
    "        Hence: y = b0 + b1*(R&DSpend) + b2*(Admin) + b3*(Marketing) + b4*(D1)\n",
    "        We have absolutely nothing to worry about the dummy variable and its trap, i.e we dont have to remove the one dummy variable.\n",
    "        \n",
    "     4. Understand what P-Value means? \n",
    "         In short, if p value is greater than significance level , it indicates that , that variable doesnot contribute much or might be negligible in predicting the target variable. So it is dropped!\n",
    "     \n",
    "     5. Building a Model\n",
    "         - Decide which attribute to keep and which to discard, i.e selecting the right attributes\n",
    "         5.1) All-in\n",
    "         5.2) Backward Elimination (Step-wise Regression) -> Preferable\n",
    "         5.3) Forward Elimination (Step-wise Regression)\n",
    "         5.4) Bi-directional Elimination (Step-wise Regression)\n",
    "         5.5) Score Comparision\n",
    "         \n",
    "         Good News! We dont really need to worry about the features to keep and discard. The class of sklearn will itself identify the features and keeps them!\n",
    "         \n",
    "[IMPORTANT]: We dont need to apply feature scaling in Multiple Linear Regression(MLR) and Simple Linear Regression, because in the equation of MLR we have coefficients that multiply to each features hence it doesn't matter that some features have high values and the others low values. The multiplier (or the coeff.) of MLR will adjust itself.\n",
    "\n",
    "[IMPORTANT]: Do we need to apply the assumptions before getting started? Well, that look pretty on theory, however, we can simply go ahead with MLR without considering the assumptions and look for the accuracy and final results. If results are bad, reject the model else great! you got the right model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"Chapter_3_50_Startups_Data.csv\")\n",
    "X = dataset.iloc[:, :-1].values \n",
    "Y = dataset.iloc[:,-1].values\n",
    "# print(X,Y)\n",
    "#iloc-> locate indexes\n",
    "# We use .values to convert it into numpy array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Categorial Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [3])],remainder='passthrough')\n",
    "X = np.array(ct.fit_transform(X))\n",
    "# we are forcing the output to be a numpy array, hence calling the np.arrar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Multiple Linear Regression model on Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[114664.42 105008.31]\n",
      " [ 90593.16  96479.51]\n",
      " [ 75692.84  78239.91]\n",
      " [ 70221.89  81229.06]\n",
      " [179790.26 191050.39]\n",
      " [171576.92 182901.99]\n",
      " [ 49753.59  35673.41]\n",
      " [102276.66 101004.64]\n",
      " [ 58649.38  49490.75]\n",
      " [ 98272.03  97483.56]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = regressor.predict(X_test)\n",
    "np.set_printoptions(precision=2)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred),1),Y_test.reshape(len(Y_test),1)),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114664.41715867177 105008.31\n",
      "90593.155316208 96479.51\n",
      "75692.8415157455 78239.91\n",
      "70221.88679652037 81229.06\n",
      "179790.25514872276 191050.39\n",
      "171576.92018520602 182901.99\n",
      "49753.58752030707 35673.41\n",
      "102276.65888936335 101004.64\n",
      "58649.37795761135 49490.75\n",
      "98272.0256113121 97483.56\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(X_test)):\n",
    "    print(y_pred[i], Y_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 1: How do I use my multiple linear regression model to make a single prediction, for example the profit of a startup with R&D Spend = 160000, Administration Spend = 130000, Marketing Spend = 300000 and State = California?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[180892.25]\n"
     ]
    }
   ],
   "source": [
    "print(regressor.predict([[1, 0, 0, 160000, 130000, 300000]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice also that the \"California\" state was not input as a string in the last column but as \"1, 0, 0\" in the first three columns. That's because of course the predict method expects the one-hot-encoded values of the state, and as we see in the second row of the matrix of features X, \"California\" was encoded as \"1, 0, 0\". And be careful to include these values in the first three columns, not the last three ones, because the dummy variables are always created in the first columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Getting the final linear regression equation with the values of the coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.85e+02  2.98e+02 -1.24e+01  7.74e-01 -9.44e-03  2.89e-02]\n",
      "49834.88507323134\n"
     ]
    }
   ],
   "source": [
    "print(regressor.coef_)\n",
    "print(regressor.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, the equation of our multiple linear regression model is:\n",
    "\n",
    "Profit=86.6×Dummy State 1−873×Dummy State 2+786×Dummy State 3−0.773×R&D Spend+0.0329×Administration+0.0366×Marketing Spend+42467.53\n",
    "\n",
    "Important Note: To get these coefficients we called the \"coef_\" and \"intercept_\" attributes from our regressor object. Attributes in Python are different than methods and usually return a simple value or an array of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
